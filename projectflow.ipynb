{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0sZwGQVvXpB"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "\n",
        "# Fresh bits of diffusers (needed for ModularPipeline/BriaFiboPipeline)\n",
        "!pip -q install --upgrade \"git+https://github.com/huggingface/diffusers\" \\\n",
        "  transformers accelerate sentencepiece safetensors pillow boltons ujson\n",
        "\n",
        "# Use PyTorch with CUDA 12.1 wheels in Colab\n",
        "!pip -q install --upgrade --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio\n",
        "\n",
        "# (Optional) If the model is gated on HF for you, uncomment to login interactively\n",
        "# from huggingface_hub import notebook_login; notebook_login()\n"
      ],
      "metadata": {
        "id": "S9dyDCNpvoNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, torch, platform\n",
        "from PIL import Image\n",
        "from diffusers import BriaFiboPipeline\n",
        "from diffusers.modular_pipelines import ModularPipeline\n",
        "\n",
        "print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available(), \"cuda ver:\", torch.version.cuda, \"gpu:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n",
        "\n",
        "# Pick dtype:\n",
        "# - Colab T4 (sm_75) doesn't support bfloat16 well â†’ use float16\n",
        "# - A100 / L4 / H100 can do bfloat16 â†’ use bfloat16\n",
        "def pick_dtype():\n",
        "    if not torch.cuda.is_available():\n",
        "        return torch.float32\n",
        "    major = torch.cuda.get_device_properties(0).major\n",
        "    # Ampere (>=8) â†’ bf16 ok, else fp16\n",
        "    return torch.bfloat16 if major >= 8 else torch.float16\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = pick_dtype()\n",
        "dtype\n"
      ],
      "metadata": {
        "id": "U_AWRS42v8PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "\n",
        "# 1) Install a stable Pillow version that works with diffusers\n",
        "!pip install --force-reinstall \"pillow==11.0.0\"\n",
        "\n",
        "# 2) Install PyTorch with CUDA 12.1 (for Colab GPU)\n",
        "!pip install --quiet --upgrade --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio\n",
        "\n",
        "# 3) Install diffusers + friends (DO NOT reinstall pillow here)\n",
        "!pip install --quiet --upgrade \\\n",
        "  \"git+https://github.com/huggingface/diffusers\" \\\n",
        "  transformers accelerate sentencepiece safetensors boltons ujson\n"
      ],
      "metadata": {
        "id": "H-dhG420xFKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "\n",
        "# 1) Pillow version that plays well with diffusers FIBO pipeline\n",
        "!pip install --quiet \"pillow==11.0.0\"\n",
        "\n",
        "# 2) diffusers (from GitHub) + friends\n",
        "!pip install --quiet --upgrade \\\n",
        "  \"git+https://github.com/huggingface/diffusers\" \\\n",
        "  transformers accelerate sentencepiece safetensors boltons ujson\n"
      ],
      "metadata": {
        "id": "broq7m17xoWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, torch\n",
        "from PIL import Image\n",
        "from diffusers import BriaFiboPipeline\n",
        "from diffusers.modular_pipelines import ModularPipeline\n",
        "\n",
        "print(\"torch:\", torch.__version__,\n",
        "      \"cuda_available:\", torch.cuda.is_available(),\n",
        "      \"cuda_version:\", torch.version.cuda,\n",
        "      \"pillow:\", Image.__version__)\n",
        "\n",
        "def pick_dtype():\n",
        "    if not torch.cuda.is_available():\n",
        "        return torch.float32\n",
        "    major = torch.cuda.get_device_properties(0).major\n",
        "    # Ampere and newer (A100 / L4 / H100, etc) â†’ bfloat16 ok; else float16\n",
        "    return torch.bfloat16 if major >= 8 else torch.float16\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = pick_dtype()\n",
        "device, dtype\n"
      ],
      "metadata": {
        "id": "Ch15zkrQx1l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m5NpmovFfpJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "O-Fmr7b4zow8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# 1) Local VLM: NL â†’ JSON\n",
        "vlm_pipe = ModularPipeline.from_pretrained(\n",
        "    \"briaai/FIBO-VLM-prompt-to-JSON\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 2) FIBO generator\n",
        "pipe = BriaFiboPipeline.from_pretrained(\n",
        "    \"briaai/FIBO\",\n",
        "    torch_dtype=dtype\n",
        ").to(device)\n",
        "\n",
        "def default_negative(existing_json: dict) -> str:\n",
        "    if existing_json.get(\"style_medium\", \"\").lower() in {\"photograph\", \"photography\", \"photo\"}:\n",
        "        return \"{'style_medium':'digital illustration','artistic_style':'non-realistic'}\"\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "3eqCmrA_yHpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rUI9NGlq1LYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "out_gen = vlm_pipe(\n",
        "    prompt=\"a studio product shot of a running shoe on matte black acrylic, soft rim light\"\n",
        ")\n",
        "json_prompt_gen = out_gen.values[\"json_prompt\"]\n",
        "\n",
        "neg = default_negative(json.loads(json_prompt_gen))\n",
        "\n",
        "generator = torch.Generator(device=device).manual_seed(12345)\n",
        "res = pipe(\n",
        "    prompt=json_prompt_gen,\n",
        "    num_inference_steps=40,\n",
        "    guidance_scale=5,\n",
        "    negative_prompt=neg,\n",
        "    generator=generator,\n",
        ")\n",
        "res.images[0].save(\"gen.png\")\n",
        "open(\"gen.json\",\"w\").write(json_prompt_gen)\n",
        "\n",
        "display(Image.open(\"gen.png\"))\n"
      ],
      "metadata": {
        "id": "fjnMaSo11MLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_ref = vlm_pipe(\n",
        "    json_prompt=json_prompt_gen,\n",
        "    prompt=\"camera angle: low angle, lens: 85mm, rim light stronger\"\n",
        ")\n",
        "json_prompt_ref = out_ref.values[\"json_prompt\"]\n",
        "\n",
        "res2 = pipe(\n",
        "    prompt=json_prompt_ref,\n",
        "    num_inference_steps=40,\n",
        "    guidance_scale=5,\n",
        "    negative_prompt=neg,\n",
        "    generator=torch.Generator(device=device).manual_seed(12345),\n",
        ")\n",
        "res2.images[0].save(\"refine.png\")\n",
        "open(\"refine.json\",\"w\").write(json_prompt_ref)\n",
        "\n",
        "display(Image.open(\"refine.png\"))\n"
      ],
      "metadata": {
        "id": "mAi79Zgv1tKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orig = Image.open(\"gen.png\")\n",
        "out_inspire = vlm_pipe(\n",
        "    image=orig,\n",
        "    prompt=\"make it futuristic with cool cyan accents\"\n",
        ")\n",
        "json_prompt_insp = out_inspire.values[\"json_prompt\"]\n",
        "\n",
        "res3 = pipe(\n",
        "    prompt=json_prompt_insp,\n",
        "    num_inference_steps=40,\n",
        "    guidance_scale=5,\n",
        "    negative_prompt=neg,\n",
        "    generator=torch.Generator(device=device).manual_seed(12345),\n",
        ")\n",
        "res3.images[0].save(\"inspire.png\")\n",
        "open(\"inspire.json\",\"w\").write(json_prompt_insp)\n",
        "\n",
        "display(Image.open(\"inspire.png\"))\n"
      ],
      "metadata": {
        "id": "22m_qtAL2A2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n"
      ],
      "metadata": {
        "id": "RUa00AOq8bIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "from diffusers import BriaFiboPipeline\n",
        "from diffusers.modular_pipelines import ModularPipeline\n",
        "\n",
        "# ---------- DEVICE & DTYPE ----------\n",
        "def pick_dtype():\n",
        "    if not torch.cuda.is_available():\n",
        "        return torch.float32\n",
        "    major = torch.cuda.get_device_properties(0).major\n",
        "    return torch.bfloat16 if major >= 8 else torch.float16\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = pick_dtype()\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# ---------- LOAD MODELS (GLOBAL SINGLETONS) ----------\n",
        "print(\"Loading VLM...\")\n",
        "vlm_pipe = ModularPipeline.from_pretrained(\n",
        "    \"briaai/FIBO-VLM-prompt-to-JSON\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Loading FIBO generator...\")\n",
        "pipe = BriaFiboPipeline.from_pretrained(\n",
        "    \"briaai/FIBO\",\n",
        "    torch_dtype=dtype\n",
        ").to(device)\n",
        "\n",
        "print(\"Models loaded.\")\n",
        "\n",
        "# ---------- UTILITIES ----------\n",
        "def default_negative(existing_json: dict) -> str:\n",
        "    if existing_json.get(\"style_medium\", \"\").lower() in {\"photograph\", \"photography\", \"photo\"}:\n",
        "        return \"{'style_medium':'digital illustration','artistic_style':'non-realistic'}\"\n",
        "    return \"\"\n",
        "\n",
        "def apply_controls_to_json(base_json: dict,\n",
        "                           camera_angle: str,\n",
        "                           focal_length: int,\n",
        "                           fov: int,\n",
        "                           key_light_style: str,\n",
        "                           palette: str):\n",
        "    \"\"\"Modify a parsed JSON prompt with UI controls.\"\"\"\n",
        "    j = dict(base_json)\n",
        "\n",
        "    # Camera & lens\n",
        "    if camera_angle:\n",
        "        j[\"camera_angle\"] = camera_angle\n",
        "    j[\"lens_focal_length\"] = f\"{focal_length}mm\"\n",
        "\n",
        "    # We don't have true FOV in schema, but you can encode it as metadata / hint\n",
        "    j.setdefault(\"camera\", {})\n",
        "    j[\"camera\"][\"field_of_view\"] = f\"{fov}deg\"\n",
        "\n",
        "    # Lighting\n",
        "    if key_light_style:\n",
        "        lighting = j.get(\"lighting\", {})\n",
        "        lighting[\"key_light\"] = key_light_style\n",
        "        j[\"lighting\"] = lighting\n",
        "\n",
        "    # Palette\n",
        "    if palette:\n",
        "        colors = [c.strip() for c in palette.split(\",\") if c.strip()]\n",
        "        if colors:\n",
        "            j[\"color_palette\"] = colors\n",
        "\n",
        "    return j\n",
        "\n",
        "def run_fibo(json_obj: dict, seed: int = 12345, steps: int = 40, guidance: float = 5.0):\n",
        "    json_str = json.dumps(json_obj)\n",
        "    neg = default_negative(json_obj)\n",
        "\n",
        "    gen = torch.Generator(device=device).manual_seed(seed)\n",
        "    out = pipe(\n",
        "        prompt=json_str,\n",
        "        num_inference_steps=steps,\n",
        "        guidance_scale=guidance,\n",
        "        negative_prompt=neg,\n",
        "        generator=gen\n",
        "    )\n",
        "    return out.images[0], json_str\n",
        "\n",
        "# ---------- GRADIO HANDLERS ----------\n",
        "def generate_from_text(prompt,\n",
        "                       camera_angle,\n",
        "                       focal_length,\n",
        "                       fov,\n",
        "                       key_light_style,\n",
        "                       palette,\n",
        "                       steps,\n",
        "                       guidance,\n",
        "                       seed):\n",
        "    \"\"\"Generate mode: NL â†’ JSON â†’ apply controls â†’ image.\"\"\"\n",
        "    if not prompt:\n",
        "        return None, None, \"Please enter a prompt.\"\n",
        "\n",
        "    # First: NL â†’ JSON via VLM\n",
        "    vlm_out = vlm_pipe(prompt=prompt)\n",
        "    base_json_str = vlm_out.values[\"json_prompt\"]\n",
        "    base_json = json.loads(base_json_str)\n",
        "\n",
        "    # Apply controls\n",
        "    final_json = apply_controls_to_json(\n",
        "        base_json,\n",
        "        camera_angle=camera_angle,\n",
        "        focal_length=focal_length,\n",
        "        fov=fov,\n",
        "        key_light_style=key_light_style,\n",
        "        palette=palette,\n",
        "    )\n",
        "\n",
        "    img, final_json_str = run_fibo(final_json, seed=seed, steps=steps, guidance=guidance)\n",
        "    pretty_json = json.dumps(final_json, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return img, pretty_json, \"\"\n",
        "\n",
        "\n",
        "def refine_existing(json_in,\n",
        "                    refine_instruction,\n",
        "                    camera_angle,\n",
        "                    focal_length,\n",
        "                    fov,\n",
        "                    key_light_style,\n",
        "                    palette,\n",
        "                    steps,\n",
        "                    guidance,\n",
        "                    seed):\n",
        "    \"\"\"Refine mode: existing JSON â†’ optional VLM refine â†’ apply controls â†’ image.\"\"\"\n",
        "    if not json_in.strip():\n",
        "        return None, None, \"Paste a JSON prompt (generated from the left tab) to refine.\"\n",
        "\n",
        "    try:\n",
        "        current_json = json.loads(json_in)\n",
        "    except Exception as e:\n",
        "        return None, None, f\"JSON parse error: {e}\"\n",
        "\n",
        "    refined = current_json\n",
        "    if refine_instruction.strip():\n",
        "        # Use VLM to refine the JSON but start from current_json\n",
        "        vlm_out = vlm_pipe(json_prompt=json.dumps(current_json),\n",
        "                           prompt=refine_instruction)\n",
        "        refined = json.loads(vlm_out.values[\"json_prompt\"])\n",
        "\n",
        "    final_json = apply_controls_to_json(\n",
        "        refined,\n",
        "        camera_angle=camera_angle,\n",
        "        focal_length=focal_length,\n",
        "        fov=fov,\n",
        "        key_light_style=key_light_style,\n",
        "        palette=palette,\n",
        "    )\n",
        "\n",
        "    img, final_json_str = run_fibo(final_json, seed=seed, steps=steps, guidance=guidance)\n",
        "    pretty_json = json.dumps(final_json, indent=2, ensure_ascii=False)\n",
        "    return img, pretty_json, \"\"\n",
        "\n",
        "\n",
        "def inspire_from_image(input_image,\n",
        "                       inspire_instruction,\n",
        "                       camera_angle,\n",
        "                       focal_length,\n",
        "                       fov,\n",
        "                       key_light_style,\n",
        "                       palette,\n",
        "                       steps,\n",
        "                       guidance,\n",
        "                       seed):\n",
        "    \"\"\"Inspire mode: image â†’ JSON â†’ apply controls â†’ image.\"\"\"\n",
        "    if input_image is None:\n",
        "        return None, None, \"Upload an image to inspire from.\"\n",
        "\n",
        "    vlm_out = vlm_pipe(image=input_image, prompt=inspire_instruction or \"\")\n",
        "    base_json = json.loads(vlm_out.values[\"json_prompt\"])\n",
        "\n",
        "    final_json = apply_controls_to_json(\n",
        "        base_json,\n",
        "        camera_angle=camera_angle,\n",
        "        focal_length=focal_length,\n",
        "        fov=fov,\n",
        "        key_light_style=key_light_style,\n",
        "        palette=palette,\n",
        "    )\n",
        "\n",
        "    img, final_json_str = run_fibo(final_json, seed=seed, steps=steps, guidance=guidance)\n",
        "    pretty_json = json.dumps(final_json, indent=2, ensure_ascii=False)\n",
        "    return img, pretty_json, \"\"\n",
        "\n",
        "\n",
        "# ---------- GRADIO UI ----------\n",
        "camera_angle_choices = [\n",
        "    \"\",            # means \"no override\"\n",
        "    \"eye-level\",\n",
        "    \"low\",\n",
        "    \"high\",\n",
        "    \"overhead\",\n",
        "    \"dutch angle\",\n",
        "]\n",
        "\n",
        "key_light_choices = [\n",
        "    \"\",\n",
        "    \"soft rim light\",\n",
        "    \"hard dramatic light\",\n",
        "    \"soft studio key\",\n",
        "    \"backlit silhouette\"\n",
        "]\n",
        "\n",
        "with gr.Blocks(title=\"FIBO Studio - Camera & JSON Control\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # ðŸŽ¥ FIBO Studio\n",
        "        **JSON-native, controllable AI camera** built on Bria FIBO.\n",
        "        - Type a prompt â†’ see the structured JSON\n",
        "        - Tweak camera, lens, FOV, lighting, palette\n",
        "        - Regenerate with full reproducibility (seed)\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            with gr.Tab(\"Generate\"):\n",
        "                prompt = gr.Textbox(label=\"Natural language prompt\", lines=3,\n",
        "                                    value=\"a studio product shot of a running shoe on matte black acrylic, soft rim light\")\n",
        "\n",
        "                gen_camera_angle = gr.Dropdown(camera_angle_choices, label=\"Camera angle override\", value=\"low\")\n",
        "                gen_focal = gr.Slider(18, 200, value=85, step=1, label=\"Lens focal length (mm)\")\n",
        "                gen_fov = gr.Slider(20, 120, value=60, step=1, label=\"Field of view (deg, hint)\")\n",
        "                gen_key_light = gr.Dropdown(key_light_choices, label=\"Key light style\", value=\"soft rim light\")\n",
        "                gen_palette = gr.Textbox(label=\"Color palette (comma-separated)\",\n",
        "                                         value=\"black, cyan accents\")\n",
        "\n",
        "                gen_steps = gr.Slider(10, 60, value=40, step=1, label=\"Steps\")\n",
        "                gen_guidance = gr.Slider(1, 15, value=5, step=0.5, label=\"Guidance scale\")\n",
        "                gen_seed = gr.Number(value=12345, precision=0, label=\"Seed\")\n",
        "\n",
        "                gen_button = gr.Button(\"Generate\")\n",
        "\n",
        "            with gr.Tab(\"Refine\"):\n",
        "                refine_json_in = gr.Textbox(label=\"Existing JSON prompt\", lines=10,\n",
        "                                            placeholder=\"Paste JSON from the Generate tab to refine...\")\n",
        "                refine_instruction = gr.Textbox(\n",
        "                    label=\"Refine instruction (e.g. 'make lighting more cinematic, stronger rim light')\", lines=2)\n",
        "\n",
        "                ref_camera_angle = gr.Dropdown(camera_angle_choices, label=\"Camera angle override\", value=\"\")\n",
        "                ref_focal = gr.Slider(18, 200, value=85, step=1, label=\"Lens focal length (mm)\")\n",
        "                ref_fov = gr.Slider(20, 120, value=60, step=1, label=\"Field of view (deg, hint)\")\n",
        "                ref_key_light = gr.Dropdown(key_light_choices, label=\"Key light style\", value=\"\")\n",
        "                ref_palette = gr.Textbox(label=\"Color palette (comma-separated)\", value=\"\")\n",
        "\n",
        "                ref_steps = gr.Slider(10, 60, value=40, step=1, label=\"Steps\")\n",
        "                ref_guidance = gr.Slider(1, 15, value=5, step=0.5, label=\"Guidance scale\")\n",
        "                ref_seed = gr.Number(value=12345, precision=0, label=\"Seed\")\n",
        "\n",
        "                refine_button = gr.Button(\"Refine\")\n",
        "\n",
        "            with gr.Tab(\"Inspire\"):\n",
        "                inspire_image = gr.Image(type=\"pil\", label=\"Inspiration image\")\n",
        "                inspire_instruction = gr.Textbox(\n",
        "                    label=\"Inspire instruction (e.g. 'make it futuristic with cool cyan accents')\", lines=2)\n",
        "\n",
        "                insp_camera_angle = gr.Dropdown(camera_angle_choices, label=\"Camera angle override\", value=\"\")\n",
        "                insp_focal = gr.Slider(18, 200, value=50, step=1, label=\"Lens focal length (mm)\")\n",
        "                insp_fov = gr.Slider(20, 120, value=60, step=1, label=\"Field of view (deg, hint)\")\n",
        "                insp_key_light = gr.Dropdown(key_light_choices, label=\"Key light style\", value=\"\")\n",
        "                insp_palette = gr.Textbox(label=\"Color palette (comma-separated)\", value=\"\")\n",
        "\n",
        "                insp_steps = gr.Slider(10, 60, value=40, step=1, label=\"Steps\")\n",
        "                insp_guidance = gr.Slider(1, 15, value=5, step=0.5, label=\"Guidance scale\")\n",
        "                insp_seed = gr.Number(value=12345, precision=0, label=\"Seed\")\n",
        "\n",
        "                inspire_button = gr.Button(\"Inspire\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            output_image = gr.Image(label=\"Output image\")\n",
        "            output_json = gr.Code(label=\"Final JSON prompt\")\n",
        "            status = gr.Markdown(label=\"Status / Messages\")\n",
        "\n",
        "    # --- Wire buttons ---\n",
        "    gen_button.click(\n",
        "        generate_from_text,\n",
        "        inputs=[\n",
        "            prompt,\n",
        "            gen_camera_angle, gen_focal, gen_fov, gen_key_light, gen_palette,\n",
        "            gen_steps, gen_guidance, gen_seed\n",
        "        ],\n",
        "        outputs=[output_image, output_json, status]\n",
        "    )\n",
        "\n",
        "    refine_button.click(\n",
        "        refine_existing,\n",
        "        inputs=[\n",
        "            refine_json_in,\n",
        "            refine_instruction,\n",
        "            ref_camera_angle, ref_focal, ref_fov, ref_key_light, ref_palette,\n",
        "            ref_steps, ref_guidance, ref_seed\n",
        "        ],\n",
        "        outputs=[output_image, output_json, status]\n",
        "    )\n",
        "\n",
        "    inspire_button.click(\n",
        "        inspire_from_image,\n",
        "        inputs=[\n",
        "            inspire_image,\n",
        "            inspire_instruction,\n",
        "            insp_camera_angle, insp_focal, insp_fov, insp_key_light, insp_palette,\n",
        "            insp_steps, insp_guidance, insp_seed\n",
        "        ],\n",
        "        outputs=[output_image, output_json, status]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "POKsRhba8gHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "comfy_workflow = {\n",
        "    \"nodes\": [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"type\": \"LoadJSON\",\n",
        "            \"title\": \"Structured JSON Prompt\",\n",
        "            \"inputs\": {},\n",
        "            \"outputs\": {\"json\": \"JSON\"},\n",
        "            \"position\": [50, 200]\n",
        "        },\n",
        "        {\n",
        "            \"id\": 2,\n",
        "            \"type\": \"BriaFIBOFromJSON\",\n",
        "            \"title\": \"FIBO Image Generator\",\n",
        "            \"inputs\": {\n",
        "                \"json_prompt\": [1, \"json\"]\n",
        "            },\n",
        "            \"outputs\": {\"image\": \"IMAGE\"},\n",
        "            \"position\": [350, 200]\n",
        "        },\n",
        "        {\n",
        "            \"id\": 3,\n",
        "            \"type\": \"ColorGrade\",\n",
        "            \"title\": \"Artist Color Grading\",\n",
        "            \"inputs\": {\n",
        "                \"image\": [2, \"image\"]\n",
        "            },\n",
        "            \"outputs\": {\"image\": \"IMAGE\"},\n",
        "            \"position\": [650, 120]\n",
        "        },\n",
        "        {\n",
        "            \"id\": 4,\n",
        "            \"type\": \"Upscale\",\n",
        "            \"title\": \"Upscale / Sharpen\",\n",
        "            \"inputs\": {\n",
        "                \"image\": [3, \"image\"]\n",
        "            },\n",
        "            \"outputs\": {\"image\": \"IMAGE\"},\n",
        "            \"position\": [950, 120]\n",
        "        },\n",
        "        {\n",
        "            \"id\": 5,\n",
        "            \"type\": \"SaveImage\",\n",
        "            \"title\": \"Final Output\",\n",
        "            \"inputs\": {\n",
        "                \"image\": [4, \"image\"]\n",
        "            },\n",
        "            \"outputs\": {},\n",
        "            \"position\": [1250, 120]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(\"comfy_fibo_workflow.json\", \"w\") as f:\n",
        "    json.dump(comfy_workflow, f, indent=2)\n",
        "\n",
        "print(\"ComfyUI workflow JSON saved.\")\n"
      ],
      "metadata": {
        "id": "a-OvoJSefL6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz\n"
      ],
      "metadata": {
        "id": "YkJklcscfNln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "dot = Digraph(comment=\"FIBO ComfyUI Graph\", format=\"png\")\n",
        "dot.attr(rankdir=\"LR\", size=\"8,5\")\n",
        "\n",
        "dot.node(\"JSON\", \"Structured JSON\\n(Camera â€¢ Lighting â€¢ Palette)\", shape=\"box\")\n",
        "dot.node(\"FIBO\", \"Bria FIBO\\n(JSON â†’ Image)\", shape=\"box\", style=\"filled\", fillcolor=\"lightblue\")\n",
        "dot.node(\"GRADE\", \"Color Grading\\n(Artist Control)\", shape=\"box\")\n",
        "dot.node(\"UPSCALE\", \"Upscale / Sharpen\", shape=\"box\")\n",
        "dot.node(\"OUT\", \"Final Image\", shape=\"box\")\n",
        "\n",
        "dot.edge(\"JSON\", \"FIBO\")\n",
        "dot.edge(\"FIBO\", \"GRADE\")\n",
        "dot.edge(\"GRADE\", \"UPSCALE\")\n",
        "dot.edge(\"UPSCALE\", \"OUT\")\n",
        "\n",
        "dot\n"
      ],
      "metadata": {
        "id": "ZaNZnIcmfS1f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}